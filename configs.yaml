base:
  n_epochs: 100
  grid_size: null
  discount_gamma: 0.99
  gae_lambda: 0.95
  logdir: 'logs'
  random_init_episodes: 5
  max_episodes: null
  actor_entropy: True

dmc_benchmark:
  random_init_episodes: 50

dmc_default:
  n_paths: 50
  n_steps: 50
  horizon: 15
  expl_amount: 0.3
  total_steps: 5e6
  action_dist: 'tanh_normal'
            
dmc_small:
  n_paths: 30
  n_steps: 30
  horizon: 10
  expl_amount: 0.0
  total_steps: 260e3
  action_dist: 'tanh_normal'

dmc_nano:
  n_paths: 5
  n_steps: 5
  horizon: 3
  expl_amount: 0.3
  total_steps: 5e6
  action_dist: 'tanh_normal'

minigrid_empty_6x6:
  n_paths: 50
  n_steps: 7
  horizon: 6
  random_init_episodes: 50
  expl_amount: 0.0
  expl_steps: 1
  total_steps: 1e6
  grid_size: 6x6
  action_dist: 'one_hot'
  max_episodes: 500

minigrid_empty_8x8:
  n_paths: 50
  n_steps: 11
  horizon: 10
  random_init_episodes: 50
  expl_amount: 0.0
  expl_steps: 1
  total_steps: 1e6
  grid_size: 8x8
  action_dist: 'one_hot'
  max_episodes: 1000

algos:
  dreamer:
    use_rewards: True
    contrastive: False

  contrastive_dreamer:
    use_rewards: True
    contrastive: True

  actinf:
    use_rewards: False
    contrastive: False

  contrastive_actinf:
    use_rewards: False
    contrastive: True
